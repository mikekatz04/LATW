{"cells": [{"cell_type": "markdown", "id": "04358995-852f-4c31-a6ba-2d6b158a012a", "metadata": {}, "source": ["# Tutorial 3: Fixed dimension MCMC with Eryn"], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "133f42eb", "metadata": {}, "outputs": [], "source": ["# # if running in google colab\n", "# !pip install eryn lisaanalysistools"]}, {"cell_type": "code", "execution_count": 1, "id": "3d7be23f-7514-4f49-b491-090b06bac51f", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from lisatools.utils.constants import *\n", "from copy import deepcopy  # can be useful"]}, {"cell_type": "markdown", "id": "fc19a943-3595-4bb3-8e7a-5175c5adc6d9", "metadata": {}, "source": ["In the third tutorial, we are going to learn how to use MCMC through `Eryn` to do our data analysis investigations. In this tutorial, we will stick with simple examples and simple signals (like in Tutorial 1). In later tutorials, we will use eryn with real GW signals. "], "outputs": []}, {"cell_type": "markdown", "id": "26a94ed1-a680-47cb-afa7-9d4d77bdbcc1", "metadata": {}, "source": ["## Task 1: build your own basic MCMC"], "outputs": []}, {"cell_type": "markdown", "id": "af8a4548-88af-40f4-86a7-75099b3b9d16", "metadata": {}, "source": ["In order to better understand MCMC and `Eryn`, we are going to start by building our own MCMC algorithm for a simple problem. We will do this with a single-dimensional Gaussian likelihood centered on ($\\mu=0$) and a unit standard deviation ($\\sigma=1$) and a uniform prior. "], "outputs": []}, {"cell_type": "code", "execution_count": 2, "id": "2a17786a-51aa-49d5-be4a-bfeee9fb44f1", "metadata": {}, "outputs": [], "source": ["def log_like_gauss(x):\n", "    return -0.5 * x ** 2 - 1/2 * np.log(2 * np.pi * 1.0) # 1.0 is sigma"]}, {"cell_type": "code", "execution_count": 3, "id": "999f58a2-9130-48c7-8c25-c757d4fa0417", "metadata": {}, "outputs": [], "source": ["x_vals = np.linspace(-10.0, 10.0, 1000)\n", "# notice the `exp` in there because we are working with the log of the likelihood.\n", "plt.plot(x_vals, np.exp(log_like_gauss(x_vals)))\n", "plt.axvline(0.0)"]}, {"cell_type": "markdown", "id": "605ace35-f8c0-4e80-82fe-8d2c1a74d2fb", "metadata": {}, "source": ["We are just going to write this into a simple loop. Gather your samples into the chain list."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "77d8f1b5-4ca7-4d0e-90a5-9128d2245ece", "metadata": {}, "outputs": [], "source": ["# special clear (for internal clearing of answers)\n", "\n", "num_steps = 100000\n", "\n", "# get random starting point\n", "current_point = np.random.uniform(-10.0, 10.0)\n", "current_likelihood = log_like_gauss(current_point)\n", "chain = []\n", "for step in range(num_steps):\n", "    # propose new point using a Gaussian distribution with standard deviation of 0.5\n", "    # hint: use current + sigma * N() where N() is a draw from a normal distribution \n", "    # (np.random.randn)\n", "\n", "\n", "    # get new likelihood\n", "\n", "\n", "    # calculate the change in posterior\n", "    # here we are using a uniform prior, so its value will not change, so for now do not\n", "    # worry about the prior.\n", "\n", "\n", "    # accept or reject\n", "    # if change in log posterior is greater than log(np.random.rand()), accept, else reject\n", "\n", "\n", "    if accept:\n", "\n", "\n", "\n", "    # append point to chain\n", "    chain.append(current_point)\n", "    "]}, {"cell_type": "markdown", "id": "a497ba73-3b01-471d-a407-9858cd504907", "metadata": {}, "source": ["Now plot the chain. You will notice that the beginnning requires a \"burn in\" phase. So, when calculating the posterior, you must remove this. There are ways to calculate how much burn in is appropriate, but we will not get into that here. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "a7240bcd-5cd8-4d58-b6bd-b4c2b7711c40", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "7cb9c5be-9ad8-4bc6-a440-31abd9d8a85c", "metadata": {}, "source": ["Plot the posterior distribution generated by the MCMC and compare it against the true Likelihood plot from above. Remember to set `density=True` when building the histogram.\n", "\n", "If we take a step back here, we realize MCMC is really just a way to draw samples from a distribution. When the distribution is simple like this example, it will usually be available already with no need to run MCMC (like in scipy). However, in our area of work, the Likelihood distribution tends to be very difficult to deal with, which is why MCMC is so useful for us. To illustrate this, we will draw samples from the same distribution available in Numpy (`np.random.randn`)."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "f45a1d77-b83a-45bb-b0b0-01778ad94c74", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "32bfb5ef-cc4f-4dd4-8a6c-d7907e471ecc", "metadata": {}, "source": ["## Task 2: Use Eryn to reproduce the above result."], "outputs": []}, {"cell_type": "markdown", "id": "0a41bc84-ea31-4bfd-8c94-93f01a758ae8", "metadata": {}, "source": ["Now use the `EnsembleSampler` in `Eryn` to reproduce the above results. If you are struggling, look at the `Eryn` tutorial. For Eryn, you will need to define a prior. Use `uniform_dist` to create a simple uniform distribution. For this example, you only need to worry about the Ensemble Sampler keyword arguments. Leave those all as default values and only enter the arguments.\n", "\n", "Useful documentation:\n", "* [EnsembleSampler](https://mikekatz04.github.io/Eryn/html/user/ensemble.html#eryn.ensemble.EnsembleSampler)\n", "* [State](https://mikekatz04.github.io/Eryn/html/user/state.html#eryn.state.State)\n", "* [uniform_dist](https://mikekatz04.github.io/Eryn/html/user/prior.html#eryn.prior.uniform_dist)\n", "* [ProbDistContainer](https://mikekatz04.github.io/Eryn/html/user/prior.html#eryn.prior.ProbDistContainer)"], "outputs": []}, {"cell_type": "code", "execution_count": 7, "id": "3a108681-76ea-45bf-8422-f571175ab0d3", "metadata": {}, "outputs": [], "source": ["# imports\n", "from eryn.ensemble import EnsembleSampler\n", "from eryn.state import State\n", "from eryn.prior import uniform_dist, ProbDistContainer"]}, {"cell_type": "markdown", "id": "e57b1a33-44a1-43ff-8392-505aa76ec270", "metadata": {}, "source": ["Initialize your sampler here. The prior initialization should take the form: `ProbDistContainer({0: prior distribution})`."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "5377b011-5c45-4490-be73-938bd9b1be5a", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "fcd5642c-57d2-4632-a40c-f295e618661f", "metadata": {}, "source": ["Now choose your starting point (one for each walker). You can draw from the prior or not. Just make sure to remove any burn in at the end. Then put the starting point into a `State` object. The key here is the start point should have shape `(1, nwalkers, 1, ndim)`. Here the two 1s are for temperatures (which we are not using right now) and the number of leaves or model count (this is for RJ). After this is complete, run the sampler.\n", "\n", "Documentation:\n", "* [run_mcmc](https://mikekatz04.github.io/Eryn/html/user/ensemble.html#eryn.ensemble.EnsembleSampler.run_mcmc)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "e8a85153-687b-4672-bbc3-3c8ae68ae65c", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "97f87737-59c8-40c9-b8d8-af6bec295c92", "metadata": {}, "source": ["Examine the output chains by using the backend:\n", "\n", "* [Backend](https://mikekatz04.github.io/Eryn/html/user/backend.html#eryn.backends.Backend)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "af0d57b3-6be6-43c1-8249-db899d24da47", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "b6b7b9ac-7fd3-495e-b0b5-68bc368f5ca9", "metadata": {}, "source": ["Compare your output chains to the injected Gaussian distribution. They should match. Make sure when you plot the histogram, you set `density=True`. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "4b8c10c4-5faa-4453-bac4-260ecd95eede", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "bdee3c52-6236-4f2c-9de2-b02cc50ddd1f", "metadata": {}, "source": ["## Task 3: Parallel Tempering"], "outputs": []}, {"cell_type": "markdown", "id": "544f5282-3622-4c06-acd4-1530483d9d2a", "metadata": {}, "source": ["Now we will add parallel tempering. In order to examine the effect of addering tempering, we are going to look at a 1D distribution with two Gaussian peaks with different weights. We have provided the log Likelihood function for this exercise. You just set the x limits to proper normalize the distribution.\n", "\n", "Parallel tempering works by supressing the Log Likelihood in comparison to the log prior: `1/T * logL + logp`. This effect lowers the peaks making it easier for higher temperature chains to traverse a low-Likelihood portion of the Likelihood surface. This helps to properly sample distributions with multiple posterior modes. \n", "\n", "We will start by sampling without tempering to see how well that does. Then we will add tempering to see the improvement. "], "outputs": []}, {"cell_type": "code", "execution_count": 13, "id": "fa0e64c2-20cd-44aa-9495-12bf758efa18", "metadata": {}, "outputs": [], "source": ["from scipy.special import logsumexp\n", "\n", "class LogLikeTwoGuass:\n", "    def __init__(self, x_min: float, x_max: float):\n", "        num_for_norm = 100000\n", "        self.norm = 1.0\n", "        x_vals = np.linspace(x_min, x_max, num_for_norm)\n", "        y_vals = np.array([np.exp(self.log_like_two_gauss(x_tmp)) for x_tmp in x_vals])\n", "        self.norm = np.trapz(y_vals, x=x_vals)\n", "        \n", "    def log_like_two_gauss(self, x):\n", "        return logsumexp(np.array([(np.log(0.2) + -0.5 * (x - 50.0) ** 2), (np.log(0.8) + -0.5 * (x + 50.0) ** 2)]), axis=0) - np.log(self.norm)"]}, {"cell_type": "code", "execution_count": 14, "id": "2f4c64cc-914f-4355-af96-d12e2c63fd7b", "metadata": {}, "outputs": [], "source": ["like = LogLikeTwoGuass(-1000.0, 1000.0)"]}, {"cell_type": "code", "execution_count": 15, "id": "eea5f8c9-fabe-45b5-b207-4033f0baa372", "metadata": {}, "outputs": [], "source": ["x_vals = np.linspace(-100.0, 100.0, 1000)\n", "# notice the `exp` in there because we are working with the log of the likelihood.\n", "plt.plot(x_vals, np.exp(like.log_like_two_gauss(x_vals)))\n", "#plt.plot(x_vals, np.exp(np.array([like.log_like_two_gauss(x_tmp) for x_tmp in x_vals])))\n"]}, {"cell_type": "markdown", "id": "bb1f5dbc-60ed-4c9b-a4b3-a1a42639dd87", "metadata": {}, "source": ["Start with a non-tempered sampler. Run it and plot the histogram over the injected Likelihood. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "0cb40971-e4dc-4130-9bd3-53b6ae31c801", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "ceb743dd-19e0-44e8-b369-90452ea08aea", "metadata": {}, "source": ["### Question\n", "\n", "The plot should not look correct. Why do you think this is? When not using tempering, what determines the number of walkers inhabiting each peak?"], "outputs": []}, {"cell_type": "markdown", "id": "47685aaf-e39e-4a9c-a28f-2a24af9030d5", "metadata": {}, "source": ["Now add temperatures by providing `tempering_kwargs` kwarg to `EnsembleSampler.\n", "\n", "Useful Documentation:\n", "* [EnsembleSampler](https://mikekatz04.github.io/Eryn/html/user/ensemble.html#eryn.ensemble.EnsembleSampler)\n", "* [TemperatureControl](https://mikekatz04.github.io/Eryn/html/user/temper.html#eryn.moves.tempering.TemperatureControl)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "39d06570-cc5e-4697-a123-4aa161f959ee", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "930254b5-490d-4475-bf08-34bfb178d653", "metadata": {}, "source": ["Sample start points again from the prior with shape `(ntemps, nwalkers, 1, ndim)`. Then run the sampler."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "0d198750-113f-424d-ad1b-71794fef41c7", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "783a61ed-b36d-4a86-a36f-5faccd026bd2", "metadata": {}, "source": ["Plot the chains over the injection distribution."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "b8524058-80f5-4077-bb36-e7761999642a", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "4a35aac1-6cd8-4a4b-b454-536b166fe30e", "metadata": {}, "source": ["Now the plots should match very weel. Can you describe the change that took place?"], "outputs": []}, {"cell_type": "markdown", "id": "4ee7f11b-d81f-4a44-ac75-12956692d1a6", "metadata": {}, "source": ["## Task 6: Add GWs!"], "outputs": []}, {"cell_type": "markdown", "id": "5b6b85fd-9df2-4faa-b4c4-7d74e80d0f99", "metadata": {}, "source": ["Now we will add GW signals in the form of the simple Sinusoid we used in the first tutorial. We will start with the waveform function. Set up the `DataResidualArray` (inject whatever parameter you would like), `SensitivityMatrix`, and `AnalysisContainer`. Remember, we do not have a response on this signal, so the sensitivity curve should be `LISASens`. "], "outputs": []}, {"cell_type": "code", "execution_count": 22, "id": "0b841b5f-e19b-4fd1-ab0f-0f87eba79ded", "metadata": {}, "outputs": [], "source": ["from lisatools.datacontainer import DataResidualArray\n", "from lisatools.analysiscontainer import AnalysisContainer\n", "from lisatools.sensitivity import SensitivityMatrix, LISASens"]}, {"cell_type": "markdown", "id": "a280f336-1543-4a52-bd65-32cd32d22895", "metadata": {}, "source": ["We will add the waveform for you."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "e1be438a-07e8-43cb-9d8b-4a04de7e5b66", "metadata": {}, "outputs": [], "source": ["def sinusoidal_waveform(A: float, f0: float, phi0: float, t: np.ndarray, **kwargs) -> [np.ndarray, np.ndarray]:\n", "    h1 = A * np.sin(2 * np.pi * (f0 * t) + phi0)\n", "    h2 = A * np.cos(2 * np.pi * (f0 * t) + phi0)\n", "    return [h1, h2]"]}, {"cell_type": "markdown", "id": "0d216154-82dc-42ae-aa21-f7251527f32b", "metadata": {}, "source": ["Generate the injection data, input it into a `DataResidualArray`, load a `SensitivityMatrix`, and store everything in an `AnalysisContainer`, including the sinusoidal waveform generator. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "de543985-7bc8-4872-a33d-9b7c39a5fda9", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "f33bc868-6b88-43c4-b61b-0e89bfa3ae09", "metadata": {}, "source": ["Calculate the SNR of the injection. (`AnalysisContainer.calculate_signal_snr`)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "34af8699-aa52-4624-ba85-464f65865998", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "f042c90f-511c-4dae-b19f-50b2315c34ce", "metadata": {}, "source": ["Biuld the prior distributions for the three parameters."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "e8c17049-1c08-4262-ab50-f5c0884ab4b8", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "344fa5dc-7c7c-460d-bc04-74e19e1a36df", "metadata": {}, "source": ["Initialize the sampler. For the Likelihood function, you can use `AnalysisContainer.eryn_likelihood_function`. For  now, we will not use tempering. This generation scheme is not efficient in terms of the Likelihood evaluation time. So, we will just run this as a quick example. You can run it longer later on and/or parallelize it with Eryn's `pool` capabilities. This basic example we are using illustrates the need for fast waveform generation capabilities. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "d20fcd42-5450-4ab5-b05f-85b698f306dd", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "ed478d01-1ed5-4620-afbd-88e2d8fbad1f", "metadata": {}, "source": ["Generate the start state and run the sampler. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "31f4302b-2d7a-4ddb-b716-e2c4c97eb1dc", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "1b1f2c1d-abab-4cb9-93f9-38374e8b0686", "metadata": {}, "source": ["You can examine the chains and/or Likelihood values if you want, but you will likely have to run it for longer to get any reasonable results. "], "outputs": []}, {"cell_type": "markdown", "id": "f28c2e74-b770-4c36-bbe6-9fd91ad7d7e6", "metadata": {}, "source": ["### Question\n", "\n", "What types of things can we do to speed up this calculation? Translate this into the waveform environment. "], "outputs": []}, {"cell_type": "markdown", "id": "214651fe-2412-4583-b946-f8d426a611e5", "metadata": {}, "source": ["## Task 4: Calculate the evidence"], "outputs": []}, {"cell_type": "markdown", "id": "9d2ad90c-d973-4c9b-be9b-3230f0b65527", "metadata": {}, "source": ["Calculate the evidence for the single-peaked Gaussian distribution using thermodynamic integration from Eryn. Use 10 walkers and 50 temperatures to start. You can play with these numbers later and observe their effect on the measurement. Use the `burn` kwarg when running `run_mcmc`. **Important**: for the `tempering_kwargs` keyword argument for `EnsembleSampler`, you must add to the dictionary: `stop_adaptation=burn`. This will adapt the temperatures during burn in and then hold them fixed while recording samples. \n", "\n", "Useful documentation:\n", "* [thermodynamic_integration_log_evidence](https://mikekatz04.github.io/Eryn/html/user/utils.html#eryn.utils.utility.thermodynamic_integration_log_evidence)"], "outputs": []}, {"cell_type": "code", "execution_count": 29, "id": "ca60f8f9-cd23-4169-97ab-5c92f2376b50", "metadata": {}, "outputs": [], "source": ["# imports\n", "from eryn.utils.utility import thermodynamic_integration_log_evidence"]}, {"cell_type": "markdown", "id": "e59dd502-57ea-4f07-ba4f-48c79788bd62", "metadata": {}, "source": ["Initialize the sampler. Run the MCMC."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "70ba5612-96fb-47b6-98cb-081c079e2e46", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "8afabb99-cf36-4c82-9455-ed6fdfbee6c8", "metadata": {}, "source": ["1) Compute the average likelihood across all samples and walkers **within** each temperature chain.\n", "2) Get the inverse temperatures (`betas`) out of the sampler backend and make sure they are the same over the entire run.\n", "3) Calculate the evidence.\n", "\n", "Useful Documentation:\n", "* [thermodynamic_integration_log_evidence](https://mikekatz04.github.io/Eryn/html/user/utils.html#eryn.utils.utility.thermodynamic_integration_log_evidence)\n", "* [Backend.get_log_like](https://mikekatz04.github.io/Eryn/html/user/backend.html#eryn.backends.Backend.get_log_like)\n", "* [Backend.get_betas](https://mikekatz04.github.io/Eryn/html/user/backend.html#eryn.backends.Backend.get_betas)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "7f9a687f-3be3-467d-96ba-e3f7497a02fb", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "bd0e4162-af73-4258-b616-c3fb1a5909c1", "metadata": {}, "source": ["## Task 5: Model Selection"], "outputs": []}, {"cell_type": "markdown", "id": "62c896f5-8b52-4550-85f8-fa1be50356ab", "metadata": {}, "source": ["Perform model comparison between a Gaussian and Cauchy pulse using thermodynamic integration of the evidence."], "outputs": []}, {"cell_type": "markdown", "id": "7a1f230e-d571-4756-8f37-4f5d99ab258d", "metadata": {}, "source": ["Here are the pulse functions and the Likelhiood. We will assume each pulse has an amplitude and mean parameter. We set the standard deviation to be 1. "], "outputs": []}, {"cell_type": "code", "execution_count": 32, "id": "fb7cc85d-aacb-4ff0-89fb-a24defdfd228", "metadata": {}, "outputs": [], "source": ["from scipy.stats import cauchy\n", "def gaussian_pulse(x, a, b):\n", "    f_x = a * np.exp(-((x - b) ** 2) / (2 * 1.0 ** 2))\n", "    return f_x\n", "\n", "def cauchy_pulse(x, a, b):\n", "    f_x = a * cauchy.pdf(x - b)\n", "    return f_x\n", "\n", "def log_like_fn(params, t, data, sigma, which_template):\n", "\n", "    pulse_gen = gaussian_pulse if which_template == \"gauss\" else cauchy_pulse\n", "    template = pulse_gen(t, *params)\n", "\n", "    ll = -0.5 * np.sum(((template - data) / sigma) ** 2, axis=-1)\n", "    return ll"]}, {"cell_type": "code", "execution_count": 33, "id": "bffaaf5c-82c6-4f68-a348-8f2fb85bf54a", "metadata": {}, "outputs": [], "source": ["t_vals = np.linspace(-10.0, 10.0, 1000)\n", "sigma = 0.2\n", "amp_true = 4.0\n", "mean_true = 0.0\n", "true_data = gaussian_pulse(t_vals, amp_true, mean_true)\n", "data = true_data + np.random.randn(*t_vals.shape) * sigma\n", "cauchy_data = cauchy_pulse(t_vals, amp_true * 3, mean_true)\n", "plt.plot(t_vals, data, label=\"data\")\n", "plt.plot(t_vals, true_data, label=\"gauss\")\n", "plt.plot(t_vals, cauchy_data, label=\"cauchy\")\n", "plt.legend()\n", "# plt.plot(x_vals, np.exp(log_like_fn()))\n", "# plt.plot(x_vals, np.exp(log_like_gauss(x_vals)))"]}, {"cell_type": "markdown", "id": "94cc0280-5d86-4dfa-bdec-4ffd488979bf", "metadata": {}, "source": ["We need to run separate sampler objects for each model. \n", "\n", "The priors are identical really for the two models. The amplitude prior should span the injection values. The mean prior should span the domain of time.\n", "\n", "Initialize the priors for each, the sampler for each, sample a start point for each, and run both samplers with the setup we used above. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "bcdc0c27-45dc-4382-986a-5ed8e27d555f", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "d555f8c2-84f8-40b4-bc39-7576ee1586ec", "metadata": {}, "source": ["Repeat the calculation from above for both models. Then find Bayes Factor."], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "033cccee-c5d2-4792-abf9-df3f72f69697", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "5b87f58f-79ac-4cad-bb60-13720b68e791", "metadata": {}, "source": ["### Question\n", "\n", "If we change the noise, what effect will this have on our results?"], "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.5"}}, "nbformat": 4, "nbformat_minor": 5}