{"cells": [{"cell_type": "markdown", "id": "d730ae19-3c50-4ded-8165-20ba0109091c", "metadata": {}, "source": ["# Tutorial 6: Galactic Binaries & RJMCMC"], "outputs": []}, {"cell_type": "markdown", "id": "fcfa353f-a37f-4fae-b288-a3aa754a5f78", "metadata": {}, "source": ["In the sixth tutorial, we will examine Galactic Binary waveforms. We will then use them in fixed-dimensional MCMC and then in RJMCMC. We use RJMCMC to perform model selection on the number of sources in the data. "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "b87b97ad", "metadata": {}, "outputs": [], "source": ["# # if running in google colab\n", "# !apt-get install liblapacke-dev libgsl-dev\n", "# !pip install eryn lisaanalysistools\n", "# !pip install gbgpu"]}, {"cell_type": "code", "execution_count": 1, "id": "de65c1f4-a35b-45f8-8afd-b7dd982eb929", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from lisatools.utils.constants import *\n", "from copy import deepcopy  # can be useful\n", "from lisatools.utils.constants import *\n", "from lisatools.sensitivity import get_sensitivity"]}, {"cell_type": "markdown", "id": "19546ba6-e9d9-4330-85e6-c55a8047f24a", "metadata": {}, "source": ["## Task 1: Build and plot a Galacic Binary waveform using `GBGPU`"], "outputs": []}, {"cell_type": "markdown", "id": "124145ca-adc8-4537-a0b1-021df364f2dd", "metadata": {}, "source": ["We will start by generating Galactic Binary waveforms with `GBGPU`. Pick reasonable parameters, build a waveform and plot it against the LISA A channel TDI Sensitivity Curve (`A1TDISens`) in the characteristic strain representation. You can access the information after waveform generation as attributes on the class. This may be updated in the future.\n", "\n", "Useful documentation:\n", "* [GBGPU](https://mikekatz04.github.io/GBGPU/html/user/main.html#gbgpu.gbgpu.GBGPU)"], "outputs": []}, {"cell_type": "code", "execution_count": 2, "id": "e4cacc05-acdd-4ce5-b36b-e08ea0e414bf", "metadata": {}, "outputs": [], "source": ["# imports\n", "from gbgpu.gbgpu import GBGPU"]}, {"cell_type": "code", "execution_count": 3, "id": "516f4c0a-9c61-493b-b458-27c7e850eeee", "metadata": {}, "outputs": [], "source": ["gb = GBGPU()"]}, {"cell_type": "code", "execution_count": null, "id": "dc107e26-1065-44f2-ba6f-0551255acde8", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "5838038d-3bf6-4707-8a3b-1b95e02f1ea5", "metadata": {}, "source": ["## Task 2: Run an MCMC over a single GB source"], "outputs": []}, {"cell_type": "markdown", "id": "3c627600-1f66-4745-916a-c3b05187e2ce", "metadata": {}, "source": ["Run a fixed-dimensional MCMC run with a chosen GB source. Fix the sky location for now to simplify the problem computationally (this is especially important for the next section on RJ with GBs). So, you will sample over 6 of the 8 parameters. Discuss or think about reasonable priors for these parameters and how you would determine that. For simplicity, we recommend using tightly (but not too tightly) bound uniform distributions for this example setup.\n", "\n", "There is a faster `get_ll` method on the `GBGPU` class. However, it may be easier to use the full `AnalysisContainer` setup. This will make the RJ part more straight forward, but is not actually ideal for fixed-dimensional MCMC on GBs. \n", "\n", "After the run is complete, plot the posterior distribution with `chainconsumer` or `corner`. "], "outputs": []}, {"cell_type": "code", "execution_count": 5, "id": "53357f1e-0504-41b7-88ea-2bdad6e34696", "metadata": {}, "outputs": [], "source": ["#imports\n", "from eryn.prior import uniform_dist, ProbDistContainer"]}, {"cell_type": "code", "execution_count": null, "id": "54777c5f-ebd0-4b0c-87d8-a930f0d8215f", "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "id": "7293a371-e723-4156-909b-05b4b8b2d022", "metadata": {}, "source": ["## Task 3: RJ with GBs"], "outputs": []}, {"cell_type": "markdown", "id": "2fe339a2-a7fa-4c19-8bfc-85ad7938a885", "metadata": {}, "source": ["Our final task will be to run RJMCMC on a few close Galactic Binaries. The key component here is the \"global\" Likelihood function. Work to build a function that takes from Eryn and adjustable length array of templates to be summed into a global template prior to the Likelihood computations. This will be a bit tedious, but is very important for understanding this process. \n", "\n", "There is another nuance in this problem that must be dealt with to get this all to work. In the fixed-dimensional case with one binary, the default stretch proposal is effectively invariant to the scale along each dimension as there is no mixing of dimensional information when making a  proposal, $\\vec{Y} = \\vec{X}_j + z\\left(\\vec{X}_i - \\vec{X}_j\\right)$. The default `GaussianMove` that we used in tutorial 5 requires an inversion of the covariance matrix. If we sample in the parameters we used above ($A$, $f_0$, $\\dot{f}$, etc.), the scale differences between parameters will cause numerical issues with matrix inversion and multiplication. \n", "\n", "There is a small variety of ways to deal with this. Here are two possibilities:\n", "\n", "1) You can log scale and reduce each parmeter so they are all of order 1. This would involve changing the priors and making sure you include this conversion in your Likelihood function. You can use `eryn.utils.TransformContainer` to do this conversion if you would like.\n", "2) You can create your own proposal where you assume a diagonal covariance and generate the information yourself. I have chosen to take this route for this example. **Hint**: the Eryn tutorial has an example of this. \n", "\n", "If you really want to get fancy: \n", "\n", "In the setup described, every source that currently has `inds=True` will be moved together. That means, if a given walker has 5 sources, all 5 sources' parameters will change at the same time. This can hurt the acceptance fraction of these moves. In reality, you may want to use Gibbs sampling to sample one or a few sources at once. You can accomplish this using the `gibbs_sampling_setup` kwarg for `eryn.moves.Move`. \n", "\n", "Useful documentation:\n", "\n", "* [MHMove](https://mikekatz04.github.io/Eryn/html/user/moves.html#eryn.moves.MHMove)\n", "* [TransformContainer](https://mikekatz04.github.io/Eryn/html/user/utils.html#eryn.utils.TransformContainer)\n", "* [Move](https://mikekatz04.github.io/Eryn/html/user/moves.html#eryn.moves.Move)\n", "\n", "If you can run the sampler and confirm the Likelihoods are working, then consider this completed. The time alloted for the tutorial and the overall setup needed to run this RJ setup correclty require a lot more runtime for reasonable results. So, you can plot what comes out, but it will become more accurate as your run the sampler longer. "], "outputs": []}, {"cell_type": "code", "execution_count": 17, "id": "52de2a79-2bdb-412e-aa52-37a98dd75fbf", "metadata": {}, "outputs": [], "source": ["# imports\n", "from eryn.moves import MHMove"]}, {"cell_type": "code", "execution_count": null, "id": "7c345b36-5269-4847-a73d-890eea9d2cba", "metadata": {}, "outputs": [], "source": ["\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.1"}}, "nbformat": 4, "nbformat_minor": 5}