{"cells": [{"cell_type": "markdown", "id": "7a9e6e4d-4ec2-4497-aeaf-352495f8caa5", "metadata": {}, "source": ["# LATW Challenge Problem: Mini global fit"], "outputs": []}, {"cell_type": "markdown", "id": "8ac3a2ed-2c1f-43c5-878a-ec05064ca60d", "metadata": {}, "source": ["In this challenge problem, you will combine what you have learned from the tutorials to build a miniature global fit pipeline. We will combine 1 MBHB with ~8 Galactic binaries. The Galactic binaries will be split in two groups with the groups separated in frequency so they are non-interacting. The goal is to fit for the MBHB and for the GBs while characterizing the uncertainty in the GB model count and parameters. "], "outputs": []}, {"cell_type": "code", "execution_count": 1, "id": "161d1306-6361-4530-997a-1e17445eb4e2", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from lisatools.utils.constants import *\n", "from copy import deepcopy  # can be useful\n", "from lisatools.utils.constants import *"]}, {"cell_type": "markdown", "id": "fa60473f-9e5e-4453-ba5d-b068590e1319", "metadata": {}, "source": ["## Generate the data"], "outputs": []}, {"cell_type": "code", "execution_count": 2, "id": "a02817f6-4f61-4f5a-bf3b-d96b411e7b5d", "metadata": {}, "outputs": [], "source": ["# waveform setup\n", "from bbhx.waveformbuild import BBHWaveformFD\n", "from lisatools.sensitivity import get_sensitivity, AE1SensitivityMatrix\n", "from gbgpu.gbgpu import GBGPU\n", "gb = GBGPU()\n", "tdi_wave_gen = BBHWaveformFD()"]}, {"cell_type": "markdown", "id": "5da257b1-7662-4f7f-951a-a1b9ed572936", "metadata": {}, "source": ["### MBHB injection parameters and waveform "], "outputs": []}, {"cell_type": "code", "execution_count": 3, "id": "b5dd4e77-923d-4e4c-9103-9a3d86bb4af9", "metadata": {}, "outputs": [], "source": ["# mbh injection parameters\n", "m1 = 2e6\n", "m2 = 7e5\n", "chi1 = 0.5\n", "chi2 = 0.7\n", "dist = 15 * 1e9 * PC_SI\n", "phi_ref = 0.6\n", "f_ref = 0.0\n", "inc = np.pi / 8\n", "lam = 3.4\n", "beta = -0.7\n", "psi = np.pi/4\n", "t_ref = 1e6 # seconds\n", "\n", "length = 1024\n", "\n", "# setup data holders\n", "Tobs = YRSID_SI / 4  # 3 month\n", "dt = 10.0  # sec\n", "N = int(Tobs / dt)\n", "Tobs = N * dt\n", "\n", "freqs = np.fft.rfftfreq(N, dt)\n", "df = freqs[1] - freqs[0]\n", "AET = tdi_wave_gen(\n", "    m1,\n", "    m2, \n", "    chi1,\n", "    chi2,\n", "    dist, \n", "    phi_ref,\n", "    f_ref, \n", "    inc,\n", "    lam,\n", "    beta,\n", "    psi,\n", "    t_ref,\n", "    length=1024, \n", "    combine=False,  # TODO: check this\n", "    direct=False,\n", "    fill=True,\n", "    squeeze=True,\n", "    freqs=freqs\n", ")"]}, {"cell_type": "markdown", "id": "693c66f1-e328-48b5-9ee1-bcb8ea24f99b", "metadata": {}, "source": ["### GBs parameters and waveforms"], "outputs": []}, {"cell_type": "code", "execution_count": 4, "id": "c9097e27-c738-47bd-a1fb-dda66062df10", "metadata": {}, "outputs": [], "source": ["injection_binaries = np.asarray([\n", "    [8.232131e-22, 0.0040004234, 1.5e-17, 0.0, 1.34248932, 3.0121342, 0.8923484, 0.034323, 1.023234],\n", "    [7.823497204e-22, 0.00400087123489, 0.1e-17, 0.0, 3.47234293, 2.787263432, 1.2349823, 3.093492342, 0.9323423],\n", "    [9.234809e-22, 0.004000923048231, 0.9e-17, 0.0, 0.2312012, 0.23423874, 3.11212101, 4.230948234, 0.012312],\n", "    [2.93209832e-22, 0.00500324293432, 3e-17, 0.0, 0.7459203, 0.485720246, 0.99998523423, 2.958929324, -0.3243432],\n", "    [3.9723498273e-22, 0.005002349823742, 2e-17, 0.0, 4.23498234, 1.823487234, 0.12987545, 2.01238923, -0.92342234],\n", "    [1.87234e-22, 0.00500823423, 4e-17, 0.0, 5.8303875067, 2.1823432, 2.98523943, 3.0324923, -0.34598345],\n", "    [8.2039823423e-22, 0.0050094359834, 5e-17, 0.0, 4.23498734, 1.3340398, 1.9283423, 3.3402934823, 0.6535098324],\n", "])\n", "\n", "gb.run_wave(*injection_binaries.T, N=128, T=Tobs, dt=dt)\n", "Agb = gb.A.copy()\n", "Egb = gb.E.copy()\n", "start_inds = gb.start_inds.copy()"]}, {"cell_type": "code", "execution_count": 5, "id": "5a4e1fd7-1c9d-4c33-9654-4a07293f0268", "metadata": {"scrolled": true}, "outputs": [], "source": ["sens_mat = AE1SensitivityMatrix(freqs, model=\"sangria\", stochastic_params=(YRSID_SI,))\n", "\n", "data = [np.zeros_like(freqs, dtype=complex), np.zeros_like(freqs, dtype=complex)]\n", "data[0] += AET[0, 0]\n", "data[1] += AET[0, 1]\n", "for i in range(Agb.shape[0]):\n", "    start_ind = start_inds[i]\n", "    A = Agb[i]\n", "    E = Egb[i]\n", "\n", "    data[0][start_ind: start_ind + Agb.shape[1]] += A\n", "    data[1][start_ind: start_ind + Agb.shape[1]] += E\n", "\n", "plt.loglog(freqs, np.abs(data[0]) ** 2 * 2 * df, label=\"data\")\n", "# plt.loglog(freqs, np.abs(AET[0, 0]) ** 2 * 2 * df, label=\"MBH\")\n", "\n", "# for i in range(Agb.shape[0]):\n", "#     start_ind = start_inds[i]\n", "#     A = Agb[i]\n", "#     lab = \"GBs\" if i == 0 else None\n", "#     plt.loglog(freqs[start_ind: start_ind + Agb.shape[1]], np.abs(A) ** 2 * 2 * df, c=\"C4\", ls='--', label=lab)\n", "\n", "plt.loglog(freqs, sens_mat.sens_mat[0], c='C1', label=\"PSD\")\n", "plt.xlim(3.99e-3,4.01e-3)\n", "plt.ylim(1e-50, 1e-37)\n", "plt.legend()"]}, {"cell_type": "markdown", "id": "3a68d5d0-5ec7-4292-af1c-baa5d790768f", "metadata": {}, "source": ["## A Mini Global Fit"], "outputs": []}, {"cell_type": "markdown", "id": "500dc7ee-6b32-4105-9727-3fed122687bd", "metadata": {}, "source": ["Now you will build up a global fit pipeline to fit that data. There are many, many ways to do this. The answer key provided is just one of them. \n", "\n", "Some hints:\n", "\n", "* Likelihood: I recommend building an initial Likelihood function that concentrates on gettings the computation right rather than speed. Once, it works, you can run it longer or improve the bottleneck later on. \n", "* Remember to implement an in-model proposal for the GBs because the stretch proposal will not work.\n", "* I recommend using fixed-dimensional sampling on the MBHB. Use the regular stretch move for this.\n", "* You will have to decide how to deal with the two different groups of GBs. I recommend using two separate models or \"branches\" in the sampler with different priors. Their priors would differ in their frequency and frequency derivative.\n", "* I recommend using the `eryn.utils.TransformContainer` and the `eryn.utils.PeriodicContainer`.\n", "* Look at Tutorials 4 and 6 for inspiration. \n", "  "], "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "6510c128-0d34-43a3-98ce-fb6245d69898", "metadata": {}, "outputs": [], "source": ["\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.1"}}, "nbformat": 4, "nbformat_minor": 5}